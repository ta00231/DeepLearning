# 四、改善深层神经网络 1004

## 1、训练，验证，测试集（Train/Dev/Test sets）

![1004_05.png](attachment:1004_05.png)

数据集分为训练集、验证集、测试集

小数据划分一般按照传统比例7/3或6/2/2即可

百万级的大数据训练集占多数

#### 相关注意

验证集、测试集尽量处于同一个分布

没有测试集也可以，测试集主要用于做出无偏估计

## 2、偏差与方差（Bias/Variance）

高偏差：欠拟合，训练集误差很大，但与验证集误差相差无几  
高方差：过拟合，训练集误差很小，且与验证集误差相差很大

有两点需要注意：

  第一点，明确是什么问题：高偏差还是高方差。  
  高偏差和高方差是两种不同的情况，通常会用训练验证集来诊断算法是否存在偏差或方差问题，然后根据结果选择尝试部分方法。  
举个例子，如果算法存在高偏差问题，准备更多训练数据其实也没什么用处，至少这不是更有效的方法，所以大家要清楚存在的问题是偏差还是方差，还是两者都有问题，明确这一点有助于我们选择出最有效的方法。

第二点，权衡偏差和方差进行调整。  
在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据，假定是这样，那么，只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时减少方差。

## 3、方法之正则化 Regularization

过拟合问题---高方差  
解决方法：1、正则化  2、准备更多数据

由于数据的成本高，通常先使用正则化去避免一些过拟合问题和网络误差

#### 正则化定义

凡是能减少过拟合即泛化误差的方法，可称为正则化方法

#### 正则化原理

1）神经网络训练目的是求损失函数值最小，而达到近似最小时，获得的   w 和 b 的值是不同的，相当于获得了一个集合；  
2）而在脱离训练集后，加入新的数据，可能由于参数的不同，导致误差   和噪声形成不同程度的放大，导致结果出错；  
3）于是要控制参数，对损失函数值最小的参数集合里的 w 和 b 进行筛   选，划定一个可行区域，故避免过拟合问题，即方法：正则化。

但参数 w 和 b 对拟合曲线影响不同，w权重 影响形状和高低位，而b偏差 主要影响曲线的平移，所以正则化主要是对 w权重 的约束。

#### 正则化方法：L1 & L2 正则化

##### 范数概念

L2范数：高维勾股定理求欧氏距离

![1010_01.png](attachment:1010_01.png)

L1范数：位置取绝对值相加

![1010_02.png](attachment:1010_02.png)

Lp范数：当且仅当p>=1时，构成的集合才是凸集  
使用L1和L2就是利用其凸函数的性质，凸优化问题

##### 相关范围划定

![1010_03.png](attachment:1010_03.png)

W 到原点的距离是≤ C 的，即约束条件

下方是拉格朗日乘数法的表示方式：

![1010_04.png](attachment:1010_04.png)

补充：拉格朗日乘数法帮助我们在具有约束条件下，即从一个特定范围内求解函数的最值和极值问题，式子如下（反观上方，可找到对应的项）

![1010_05.png](attachment:1010_05.png)

拉格朗日乘数法例题与其图像

![1010_06.png](attachment:1010_06.png)

绕回来得到最终神经网络用到的式子：

![1010_07.png](attachment:1010_07.png)

求 W 问题，即求梯度，故常数可以不看，则绿色与红色在求解 W 上等价

##### L1 与 L2 的不同

![1010_08.png](attachment:1010_08.png)

L1的稀疏性从右图可以看出，坐标轴上可以有值，即通过λ的选取，会导致传入神经网络的一些特征起作用，一些特征不起作用，将特征去耦合；
L2主要是对 W权重 的约束调整更为明显。

#### 正则化方法：Dropout 正则化 （随即失活）

![1010_09.png](attachment:1010_09.png)

我们
复制这个神经网络，dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率。假
设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率
都是 0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后
得到一个节点更少，规模更小的网络，然后用 backprop 方法进行训练。

失活后，要确保激活函数得到的数值期望值不变，要进行反向随即失活

##### Inverted dropout 反向随机失活

![1010_10.png](attachment:1010_10.png)

我们假设第三隐藏层上有 50 个单元或
50 个神经元，在一维上𝑎
[3]是 50，我们通过因子分解将它拆分成50 × 𝑚维的，保留和删除它
们的概率分别为 80%和 20%，这意味着最后被删除或归零的单元平均有 10（50×20%=10）
个，现在我们看下𝑧
[4]，𝑧
[4] = 𝑤[4]𝑎
[3] + 𝑏
[4]，我们的预期是，𝑎
[3]减少 20%，也就是说𝑎
[3]中
有 20%的元素被归零，为了不影响𝑧
[4]的期望值，我们需要用𝑤[4]𝑎
[3]/0.8，它将会修正或弥
补我们所需的那 20%，𝑎
[3]的期望值不会变，划线部分就是所谓的 dropout 方法。

每层的keeppro可能不同

代价函数J不能明确定义了，计算机视觉用的较多

#### 正则化方法：其他方式

##### 数据扩增

图像处理：反转、裁剪

文字处理：强处理，扭曲文字

##### Early Stopping

![1010_11.png](attachment:1010_11.png)

原先工作分开做

1）最小化损失函数，选出 w 和 b 的集合；  
利用梯度下降法、Momentum、RMSprop、Adam等

2）处理拟合问题，正交化；

early stopping 的主要缺点就是你不能独立地处理这两个问题，因为提早停
止梯度下降，也就是停止了优化代价函数𝐽，因为现在你不再尝试降低代价函数𝐽，所以代价
函数𝐽的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个
问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。

如果不用 early stopping，另一种方法就是𝐿2正则化，训练神经网络的时间就可能很长。
我发现，这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试
很多正则化参数𝜆的值，这也导致搜索大量𝜆值的计算代价太高。

Early stopping 的优点是，只运行一次梯度下降，你可以找出𝑤的较小值，中间值和较大
值，而无需尝试𝐿2正则化超级参数𝜆的很多值。

